{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:1684: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = infer_fill_value(value)\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>labelname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799994</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205009</td>\n",
       "      <td>Thu Jun 25 10:28:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dandykim</td>\n",
       "      <td>Sick  Spending my day laying in bed listening ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799995</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205038</td>\n",
       "      <td>Thu Jun 25 10:28:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bigenya</td>\n",
       "      <td>Gmail is down?</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799996</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205473</td>\n",
       "      <td>Thu Jun 25 10:28:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LeeLHoke</td>\n",
       "      <td>rest in peace Farrah! So sad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799997</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205574</td>\n",
       "      <td>Thu Jun 25 10:28:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>davidlmulder</td>\n",
       "      <td>@Eric_Urbane Sounds like a rival is flagging y...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799998</th>\n",
       "      <td>0</td>\n",
       "      <td>2329205794</td>\n",
       "      <td>Thu Jun 25 10:28:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tpchandler</td>\n",
       "      <td>has to resit exams over summer...  wishes he w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799999 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                      date     query  \\\n",
       "0           0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1           0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2           0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3           0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4           0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...       ...         ...                           ...       ...   \n",
       "799994      0  2329205009  Thu Jun 25 10:28:28 PDT 2009  NO_QUERY   \n",
       "799995      0  2329205038  Thu Jun 25 10:28:28 PDT 2009  NO_QUERY   \n",
       "799996      0  2329205473  Thu Jun 25 10:28:30 PDT 2009  NO_QUERY   \n",
       "799997      0  2329205574  Thu Jun 25 10:28:30 PDT 2009  NO_QUERY   \n",
       "799998      0  2329205794  Thu Jun 25 10:28:31 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 name                                               text  \\\n",
       "0       scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "1            mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "2             ElleCTF    my whole body feels itchy and like its on fire    \n",
       "3              Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "4            joy_wolf                      @Kwesidei not the whole crew    \n",
       "...               ...                                                ...   \n",
       "799994       dandykim  Sick  Spending my day laying in bed listening ...   \n",
       "799995        bigenya                                    Gmail is down?    \n",
       "799996       LeeLHoke                      rest in peace Farrah! So sad    \n",
       "799997   davidlmulder  @Eric_Urbane Sounds like a rival is flagging y...   \n",
       "799998     tpchandler  has to resit exams over summer...  wishes he w...   \n",
       "\n",
       "       labelname  \n",
       "0       negative  \n",
       "1       negative  \n",
       "2       negative  \n",
       "3       negative  \n",
       "4       negative  \n",
       "...          ...  \n",
       "799994  negative  \n",
       "799995  negative  \n",
       "799996  negative  \n",
       "799997  negative  \n",
       "799998  negative  \n",
       "\n",
       "[799999 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('1.6 Million.csv',encoding='latin-1',on_bad_lines='skip')\n",
    "df.columns = [\"label\",\"\", \"date\", \"query\", \"name\",\"text\"]\n",
    "#df.drop(df.index[20000:800000], inplace=True)\n",
    "#df.drop(df.index[40000:900000], inplace=True)\n",
    "for i in df.index:\n",
    "    df.at[i, 'label'] = 1 if df.at[i, 'label'] == 4 else 0\n",
    "    \n",
    "df1 = df[df['label'] == 1]\n",
    "for i in df1.index:\n",
    "    df1.at[i, 'labelname'] = \"positive\" if df1.at[i, 'label'] == 1 else \"negative\"\n",
    "df1.to_csv('positive_tweets1.csv')\n",
    "\n",
    "df2 = df[df['label'] == 0]\n",
    "for i in df2.index:\n",
    "    df2.at[i, 'labelname'] = \"negative\" if df2.at[i, 'label'] == 0 else \"positve\"\n",
    "df2.to_csv('negative_tweets1.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [I, LOVE, @, Health4UandPets, u, guys, r, the,...\n",
       "1         [im, meeting, up, with, one, of, my, besties, ...\n",
       "2         [@, DaRealSunisaKim, Thanks, for, the, Twitter...\n",
       "3         [Being, sick, can, be, really, cheap, when, it...\n",
       "4         [@, LovesBrooklyn2, he, has, that, effect, on,...\n",
       "                                ...                        \n",
       "799995    [Just, woke, up, ., Having, no, school, is, th...\n",
       "799996    [TheWDB.com, -, Very, cool, to, hear, old, Wal...\n",
       "799997    [Are, you, ready, for, your, MoJo, Makeover, ?...\n",
       "799998    [Happy, 38th, Birthday, to, my, boo, of, alll,...\n",
       "799999    [happy, #, charitytuesday, @, theNSPCC, @, Spa...\n",
       "Name: text, Length: 800000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df1 = pd.read_csv('positive_tweets1.csv',encoding='latin-1',on_bad_lines='skip')\n",
    "df2 = pd.read_csv('negative_tweets1.csv',encoding='latin-1',on_bad_lines='skip')\n",
    "\n",
    "positive_tokens = df1['text'].apply(word_tokenize)\n",
    "negative_tokens = df2['text'].apply(word_tokenize)\n",
    "positive_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@', 'JJ'),\n",
       " ('DaRealSunisaKim', 'NNP'),\n",
       " ('Thanks', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Twitter', 'NNP'),\n",
       " ('add', 'NN'),\n",
       " (',', ','),\n",
       " ('Sunisa', 'NNP'),\n",
       " ('!', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('got', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('meet', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('once', 'RB'),\n",
       " ('at', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('HIN', 'NNP'),\n",
       " ('show', 'NN'),\n",
       " ('here', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('DC', 'NNP'),\n",
       " ('area', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('sweetheart', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "position_tag = pos_tag(positive_tokens[2])\n",
    "position_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'DaRealSunisaKim',\n",
       " 'Thanks',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Twitter',\n",
       " 'add',\n",
       " ',',\n",
       " 'Sunisa',\n",
       " '!',\n",
       " 'I',\n",
       " 'get',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'you',\n",
       " 'once',\n",
       " 'at',\n",
       " 'a',\n",
       " 'HIN',\n",
       " 'show',\n",
       " 'here',\n",
       " 'in',\n",
       " 'the',\n",
       " 'DC',\n",
       " 'area',\n",
       " 'and',\n",
       " 'you',\n",
       " 'be',\n",
       " 'a',\n",
       " 'sweetheart',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma = []\n",
    "for word, tag in position_tag:\n",
    "    lemma.append(lemmatizer.lemmatize(word, pos = 'v'))\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'DaRealSunisaKim',\n",
       " 'Thanks',\n",
       " 'Twitter',\n",
       " 'add',\n",
       " ',',\n",
       " 'Sunisa',\n",
       " '!',\n",
       " 'I',\n",
       " 'get',\n",
       " 'meet',\n",
       " 'HIN',\n",
       " 'show',\n",
       " 'DC',\n",
       " 'area',\n",
       " 'sweetheart',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_stop = [i for i in lemma if i not in stop_words]\n",
    "without_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_list = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        token = token.lower()\n",
    "        token = re.sub(r'@[a-z0-9_]\\S+', '', token)\n",
    "        token = re.sub(r'#[a-z0-9_]\\S+', '', token)\n",
    "        token = re.sub(r'&[a-z0-9_]\\S+', '', token)\n",
    "        token = re.sub(r'[?!.+,;$Â£%&\"]+', '', token)\n",
    "        token = re.sub(r'rt[\\s]+', '', token)\n",
    "        token = re.sub(r'\\d+', '', token)\n",
    "        token = re.sub(r'\\$', '', token)\n",
    "        token = re.sub(r'rt+', '', token)\n",
    "        token = re.sub(r'https?:?\\/\\/\\S+', '', token)\n",
    "        if tag.startswith('NN'):\n",
    "            position = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            position = 'v'\n",
    "        elif tag.startswith('RB'):\n",
    "            position = 'r'\n",
    "        elif tag.startswith('JJ'):\n",
    "            position = 'a'\n",
    "        else:\n",
    "            position = 'n'\n",
    "\n",
    "        clean_list.append(lemmatizer.lemmatize(token, pos = position))\n",
    "        clean_list = [i for i in clean_list if i not in stop_words and len(i) > 0 and i != ':']\n",
    "\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_positive = list(map(clean_tokens, positive_tokens))\n",
    "clean_negative = list(map(clean_tokens, negative_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_tokens[0])\n",
    "print(clean_positive[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(tokens, status):\n",
    "    featureset = [(tweet, status) for tweet in tokens]\n",
    "    return featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_featureset = data_prepare(clean_positive, 'Positive')\n",
    "negative_featureset = data_prepare(clean_negative, 'Negative')\n",
    "\n",
    "featureset = positive_featureset + negative_featureset\n",
    "featureset[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "for x in featureset:\n",
    "    features.append(x[0])\n",
    "    labels.append(x[1])\n",
    "    \n",
    "print(features[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1, 2), sublinear_tf = True, max_features = 3000, preprocessor = ' '.join)\n",
    "vectorized_features = vectorizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.get_feature_names()\n",
    "print(vocabulary[:15])\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_features, labels, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators = 200)\n",
    "classifier = text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = twitter_samples.tokenized('tweets.20150430-223406.json')\n",
    "clean_test_tweets = list(map(clean_tokens, test_tweets))\n",
    "\n",
    "print(test_tweets[0])\n",
    "print(clean_test_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Tweets'] = clean_test_tweets\n",
    "df['Sentiment'] = classifier.predict(vectorizer.transform(list(clean_test_tweets)))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
